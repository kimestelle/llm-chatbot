{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimestelle/llm-chatbot/blob/main/aristotle_llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lNHF93tV1WP"
      },
      "source": [
        "- Last sequence in dataset was randomly taken from earlier to make even batches\n",
        "- vocab size: 1000\n",
        "- number of lines: 48780\n",
        "- sequence length: 383\n",
        "- batch size: 36"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIRk8GnaruAy",
        "outputId": "dd378452-d12a-49ce-e0df-70c02ac0ec8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "import sentencepiece as spm\n",
        "from collections import OrderedDict\n",
        "import gc\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gFZTHC38o6X"
      },
      "source": [
        "# Model Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVwCS5dM-uXV",
        "outputId": "754b59de-9231-459c-bce0-cd9e9b1badd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "MASTER_CONFIG = {\n",
        "    \"dim\": 4096,\n",
        "    \"n_layers\": 16,\n",
        "    \"n_heads\": 16,\n",
        "    \"n_kv_heads\": 4,\n",
        "    \"vocab_size\": 1000,\n",
        "    \"multiple_of\": 256,\n",
        "    \"ffn_dim_multiplier\": None,\n",
        "    \"norm_eps\": 1e-5,\n",
        "    # \"max_batch_size\": 36,\n",
        "    # \"batch_size\": 36\n",
        "    # \"max_batch_size\": 18,\n",
        "    # \"batch_size\": 18\n",
        "    \"max_batch_size\": 4,\n",
        "    \"batch_size\": 4,\n",
        "    \"max_seq_len\": 412,\n",
        "    \"seq_len\": 412,\n",
        "    \"device\": device,\n",
        "}\n",
        "\n",
        "print(MASTER_CONFIG[\"device\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjTigWzt8r1b"
      },
      "source": [
        "# Import data from SentencePiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv7ANsa89F4r",
        "outputId": "e9ae2d40-d27b-4801-e073-5670f58fc924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n"
          ]
        }
      ],
      "source": [
        "spm_model = spm.SentencePieceProcessor()\n",
        "spm_model.load('/content/drive/MyDrive/philosophy_data/a.model')\n",
        "\n",
        "pad_id = spm_model.pad_id()\n",
        "\n",
        "def decode_without_padding(sequence, spm_model):\n",
        "    filtered_sequence = [token_id for token_id in sequence if token_id != pad_id]\n",
        "    return spm_model.decode_ids(filtered_sequence)\n",
        "\n",
        "vocab_size = spm_model.get_piece_size()\n",
        "print(spm_model.decode_ids(spm_model.encode_as_ids('hello world')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlIY0bHOkQVB",
        "outputId": "49c26561-a12c-4fa7-8cd4-0bb312449a5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHLBlBe3_Qrq"
      },
      "outputs": [],
      "source": [
        "# with open('/content/drive/MyDrive/philosophy_data/padded_processed_aristotle.txt', 'r') as file:\n",
        "#     lines = file.readlines()\n",
        "\n",
        "# data = [list(map(int, line.split())) for line in lines]\n",
        "\n",
        "# dataset = torch.zeros((len(data), MASTER_CONFIG[\"seq_len\"]), dtype=torch.int64)\n",
        "\n",
        "# for i in range(len(data)):\n",
        "#     seq = data[i]\n",
        "#     dataset[i, :len(seq)] = torch.tensor(seq, dtype=torch.int64)\n",
        "\n",
        "# print(dataset.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73LaPm3MMiVd"
      },
      "source": [
        "#Generate Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIlyCPFAMkvs"
      },
      "outputs": [],
      "source": [
        "def get_batches(data, split, batch_size, config=MASTER_CONFIG):\n",
        "    print(config[\"device\"])\n",
        "\n",
        "    train = data[:int(.8 * len(data))]\n",
        "    val = data[int(.8 * len(data)): int(.9 * len(data))]\n",
        "    test = data[int(.9 * len(data)):]\n",
        "\n",
        "    if split == 'train':\n",
        "        batch_data = train\n",
        "    elif split == 'val':\n",
        "        batch_data = val\n",
        "    elif split == 'test':\n",
        "        batch_data = test\n",
        "    else:\n",
        "        raise ValueError(\"Invalid split name. Choose from 'train', 'val', or 'test'.\")\n",
        "\n",
        "    # Sample batch indices\n",
        "    ix = torch.randint(0, batch_data.size(0) - 1, (batch_size,))\n",
        "\n",
        "    # Generate x and y batches\n",
        "    x = batch_data[ix].long().to(config[\"device\"])\n",
        "    y = batch_data[ix, 1:].long().to(config[\"device\"])\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnxbzXA9FI7G"
      },
      "source": [
        "#Full Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk1tcJGlFIiU"
      },
      "outputs": [],
      "source": [
        "class Llama(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embeddings = nn.Embedding(config['vocab_size'], config['dim'])\n",
        "        self.llama_blocks = nn.Sequential(\n",
        "            OrderedDict([(f\"llama_{i}\", AttentionBlock(config)) for i in range(config['n_layers'])])\n",
        "        )\n",
        "\n",
        "        self.output_layer = nn.Linear(config['dim'], config['vocab_size'])\n",
        "        self.freqs_complex = precompute_theta_pos_frequencies(\n",
        "            head_dim=config['dim'] // config['n_heads'],\n",
        "            seq_len=config['max_seq_len'],\n",
        "            device=config['device']\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None, start_pos=0):\n",
        "        # print(f\"Input idx shape before embedding: {idx.shape}\")\n",
        "        x = self.embeddings(idx)\n",
        "        # print(f\"Shape after embedding: {x.shape}\")\n",
        "\n",
        "        for llama_block in self.llama_blocks:\n",
        "            x = llama_block(x, start_pos, self.freqs_complex)\n",
        "            x = x.detach()  # Detach here if needed\n",
        "\n",
        "        logits = self.output_layer(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # Ensure logits and targets are aligned properly\n",
        "            logits = logits[:, :targets.size(1), :].contiguous()  # Adjust logits to match the target's seq_len\n",
        "            # print(f\"Adjusted logits shape: {logits.shape}\")\n",
        "            # print(f\"Adjusted targets shape: {targets.shape}\")\n",
        "\n",
        "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
        "            return logits, loss\n",
        "        else:\n",
        "            return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvh33S4K_Ns0"
      },
      "source": [
        "# Pull data and turn into embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o1RtwctDSjx"
      },
      "outputs": [],
      "source": [
        "def precompute_theta_pos_frequencies(head_dim: int, seq_len: int, device: str, theta: float = 10000.0):\n",
        "    assert head_dim % 2 == 0, \"Dimension must be divisible by 2\"\n",
        "\n",
        "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
        "    # print(f\"theta_numerator shape: {theta_numerator.shape}\")\n",
        "\n",
        "    theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device)\n",
        "    # print(f\"theta shape: {theta.shape}\")\n",
        "\n",
        "    m = torch.arange(seq_len, device=device)\n",
        "    # print(f\"m shape (sequence length): {m.shape}\")\n",
        "\n",
        "    freqs = torch.outer(m, theta).float()\n",
        "    # print(f\"freqs shape: {freqs.shape}\")\n",
        "\n",
        "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
        "    # print(f\"freqs_complex shape: {freqs_complex.shape}\")\n",
        "\n",
        "    return freqs_complex\n",
        "\n",
        "def apply_rotary_embeddings(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
        "    # print(f\"x shape: {x.shape}\")\n",
        "    # print(f\"freqs_complex shape: {freqs_complex.shape}\")\n",
        "\n",
        "    # Reshape and ensure correct shape for complex tensor\n",
        "    try:\n",
        "        batch_size, seq_len, n_heads, head_dim = x.shape\n",
        "        x_complex = torch.view_as_complex(x.float().reshape(batch_size, seq_len, n_heads, -1, 2))\n",
        "    except ValueError as e:\n",
        "        print(f\"Error unpacking x shape: {x.shape}\")\n",
        "        raise e\n",
        "\n",
        "    original_seq_len = freqs_complex.shape[0]\n",
        "\n",
        "    if seq_len != original_seq_len:\n",
        "        if seq_len < original_seq_len:\n",
        "            freqs_complex = freqs_complex[:seq_len]\n",
        "        else:\n",
        "            repeat_factor = (seq_len + original_seq_len - 1) // original_seq_len\n",
        "            freqs_complex = freqs_complex.repeat(repeat_factor, 1)[:seq_len]\n",
        "\n",
        "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
        "\n",
        "    x_rotated = x_complex * freqs_complex\n",
        "    x_out = torch.view_as_real(x_rotated).reshape(batch_size, seq_len, n_heads, head_dim)\n",
        "\n",
        "    # print(f\"x_complex shape: {x_complex.shape}\")\n",
        "    # print(f\"freqs_complex shape: {freqs_complex.shape}\")\n",
        "\n",
        "    return x_out.type_as(x).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2IpKL978x96"
      },
      "source": [
        "#Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BkP2C-ZEywu"
      },
      "outputs": [],
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config[\"n_heads\"]\n",
        "        self.dim = config[\"dim\"]\n",
        "        self.head_dim = self.dim // self.n_heads\n",
        "        self.attention = SelfAttention(config)\n",
        "        self.feed_forward = FeedForward(config)\n",
        "        self.attention_norm = RMSNorm(config[\"dim\"], eps=config[\"norm_eps\"])\n",
        "        self.ffn_norm = RMSNorm(config[\"dim\"], eps=config[\"norm_eps\"])\n",
        "\n",
        "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
        "        # print(f\"Input x shape: {x.shape}\")\n",
        "\n",
        "        # If x has 4 dimensions, ensure they represent the expected structure\n",
        "        if len(x.shape) == 4:\n",
        "            batch_size, num_heads, seq_len, head_dim = x.shape\n",
        "            if num_heads == self.n_heads and head_dim == self.head_dim:\n",
        "                # Reshape to merge heads and sequence if necessary, keeping structure intact\n",
        "                x = x.view(batch_size, seq_len, self.dim)\n",
        "            else:\n",
        "                print(\"Shape before attention:\", x.shape)\n",
        "                raise ValueError(\"Unexpected shape in attention block.\")\n",
        "\n",
        "\n",
        "        normed_x = self.attention_norm(x)\n",
        "        # print(f\"Normed x shape (after attention norm): {normed_x.shape}\")\n",
        "\n",
        "        attention_out = self.attention(normed_x, start_pos, freqs_complex)\n",
        "        # print(f\"Attention output shape: {attention_out.shape}\")\n",
        "\n",
        "        h = x + attention_out\n",
        "        # print(f\"Residual connection output h shape: {h.shape}\")\n",
        "\n",
        "        normed_h = self.ffn_norm(h)\n",
        "        # print(f\"Normed h shape (after feed-forward norm): {normed_h.shape}\")\n",
        "\n",
        "        ff_out = self.feed_forward(normed_h)\n",
        "        # print(f\"Feed-forward output shape: {ff_out.shape}\")\n",
        "\n",
        "        out = h + ff_out\n",
        "        # print(f\"Final output shape: {out.shape}\")\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfHb0O19EEAr"
      },
      "source": [
        "## RMS Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42QWOnMmE59C"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "        # print(f\"RMSNorm initialized with dim: {dim}, eps: {eps}\")\n",
        "\n",
        "    def _norm(self, x: torch.Tensor):\n",
        "        # print(f\"Input x shape in _norm: {x.shape}\")\n",
        "        norm = x.pow(2).mean(-1, keepdim=True)\n",
        "        # print(f\"Shape after mean in _norm: {norm.shape}\")\n",
        "        norm = torch.rsqrt(norm + self.eps)\n",
        "        # print(f\"Shape after rsqrt in _norm: {norm.shape}\")\n",
        "        return x * norm\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # print(f\"Input x shape in forward: {x.shape}\")\n",
        "        normed_x = self._norm(x.float()).type_as(x)\n",
        "        # print(f\"Normed x shape in forward: {normed_x.shape}\")\n",
        "        weighted_x = self.weight * normed_x\n",
        "        # print(f\"Final output shape in forward: {weighted_x.shape}\")\n",
        "        return weighted_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmgV2i_2EGRj"
      },
      "source": [
        "## Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kWnNZevE6dZ"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Define the number of query heads (n_heads_q) and key-value heads (n_kv_heads)\n",
        "        self.n_kv_heads = config[\"n_kv_heads\"] if config[\"n_kv_heads\"] is not None else config[\"n_heads\"]\n",
        "        self.n_heads_q = config[\"n_heads\"]\n",
        "        self.n_rep = self.n_heads_q // self.n_kv_heads  # How many times to repeat K and V heads\n",
        "        self.head_dim = config[\"dim\"] // self.n_heads_q\n",
        "\n",
        "        # Linear transformations for Q, K, V, and output projection\n",
        "        self.wq = nn.Linear(config[\"dim\"], self.n_heads_q * self.head_dim, bias=False)\n",
        "        self.wk = nn.Linear(config[\"dim\"], self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wv = nn.Linear(config[\"dim\"], self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wo = nn.Linear(self.n_heads_q * self.head_dim, config[\"dim\"], bias=False)\n",
        "\n",
        "        # Initialize key and value caches\n",
        "        self.cache_k = torch.zeros((config[\"max_batch_size\"], config[\"max_seq_len\"], self.n_kv_heads, self.head_dim))\n",
        "        self.cache_v = torch.zeros((config[\"max_batch_size\"], config[\"max_seq_len\"], self.n_kv_heads, self.head_dim))\n",
        "\n",
        "    @staticmethod\n",
        "    def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "        batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
        "        if n_rep == 1:\n",
        "            return x\n",
        "        return (\n",
        "            x[:, :, :, None, :]  # (B, Seq_Len, N_KV_Heads, 1, Head_Dim)\n",
        "            .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)  # (B, Seq_Len, N_KV_Heads, N_Rep, Head_Dim)\n",
        "            .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)  # (B, Seq_Len, N_KV_Heads * N_Rep, Head_Dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        # print(f\"Input x shape: {x.shape}\")\n",
        "\n",
        "        # Compute Q, K, V\n",
        "        xq = self.wq(x)\n",
        "        xk = self.wk(x)\n",
        "        xv = self.wv(x)\n",
        "        # print(f\"xq shape after wq: {xq.shape}\")\n",
        "        # print(f\"xk shape after wk: {xk.shape}\")\n",
        "        # print(f\"xv shape after wv: {xv.shape}\")\n",
        "\n",
        "        # Reshape to add head dimensions\n",
        "        xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
        "        xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
        "        xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
        "        # print(f\"xq shape after view: {xq.shape}\")\n",
        "        # print(f\"xk shape after view: {xk.shape}\")\n",
        "        # print(f\"xv shape after view: {xv.shape}\")\n",
        "\n",
        "        # Apply rotary embeddings\n",
        "        xq = apply_rotary_embeddings(xq, freqs_complex, device=x.device)\n",
        "        xk = apply_rotary_embeddings(xk, freqs_complex, device=x.device)\n",
        "        # print(f\"xq shape after rotary embeddings: {xq.shape}\")\n",
        "        # print(f\"xk shape after rotary embeddings: {xk.shape}\")\n",
        "\n",
        "        # Update and use cache\n",
        "        self.cache_k[:batch_size, start_pos : start_pos + seq_len] = xk\n",
        "        self.cache_v[:batch_size, start_pos : start_pos + seq_len] = xv\n",
        "        # print(f\"cache_k shape after update: {self.cache_k.shape}\")\n",
        "        # print(f\"cache_v shape after update: {self.cache_v.shape}\")\n",
        "\n",
        "        # Move KV cache to the device (GPU)\n",
        "        self.cache_k = self.cache_k.to(x.device)\n",
        "        self.cache_v = self.cache_v.to(x.device)\n",
        "\n",
        "        keys = self.cache_k[:batch_size, :start_pos + seq_len]\n",
        "        values = self.cache_v[:batch_size, :start_pos + seq_len]\n",
        "        # print(f\"keys shape after cache slice: {keys.shape}\")\n",
        "        # print(f\"values shape after cache slice: {values.shape}\")\n",
        "\n",
        "        # Repeat K and V heads to match Q heads\n",
        "        keys = self.repeat_kv(keys, self.n_rep)\n",
        "        values = self.repeat_kv(values, self.n_rep)\n",
        "        # print(f\"keys shape after repeat_kv: {keys.shape}\")\n",
        "        # print(f\"values shape after repeat_kv: {values.shape}\")\n",
        "\n",
        "        # Transpose to match expected shapes for matmul\n",
        "        xq = xq.transpose(1, 2)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "        # print(f\"xq shape after transpose: {xq.shape}\")\n",
        "        # print(f\"keys shape after transpose: {keys.shape}\")\n",
        "        # print(f\"values shape after transpose: {values.shape}\")\n",
        "\n",
        "        # Attention mechanism\n",
        "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "        # print(f\"scores shape after matmul: {scores.shape}\")\n",
        "\n",
        "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "        # print(f\"scores shape after softmax: {scores.shape}\")\n",
        "\n",
        "        output = torch.matmul(scores, values)\n",
        "        # print(f\"output shape after matmul: {output.shape}\")\n",
        "\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
        "        # print(f\"output shape after transpose and view: {output.shape}\")\n",
        "\n",
        "        final_output = self.wo(output)\n",
        "        # print(f\"Final output shape: {final_output.shape}\")\n",
        "\n",
        "        return final_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWHvMH17EI-h"
      },
      "source": [
        "## Feed Forward Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6ZqFNFtH207"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        hidden_dim = 4 * config[\"dim\"]\n",
        "        hidden_dim = int(2 * hidden_dim / 3)\n",
        "        if config[\"ffn_dim_multiplier\"] is not None:\n",
        "            hidden_dim = int(config[\"ffn_dim_multiplier\"] * hidden_dim)\n",
        "            hidden_dim = config[\"multiple_of\"] * ((hidden_dim + config[\"multiple_of\"] - 1) // config[\"multiple_of\"])\n",
        "\n",
        "        self.w1 = nn.Linear(config[\"dim\"], hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, config[\"dim\"], bias=False)\n",
        "        self.w3 = nn.Linear(config[\"dim\"], hidden_dim, bias=False)\n",
        "\n",
        "        # print(f\"FeedForward initialized with input dim: {config['dim']} and hidden dim: {hidden_dim}\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # print(f\"Input x shape: {x.shape}\")\n",
        "\n",
        "        swish = F.silu(self.w1(x))\n",
        "        # print(f\"Shape after w1 and SiLU activation: {swish.shape}\")\n",
        "\n",
        "        x_V = self.w3(x)\n",
        "        # print(f\"Shape after w3: {x_V.shape}\")\n",
        "\n",
        "        x = swish * x_V\n",
        "        # print(f\"Shape after element-wise multiplication: {x.shape}\")\n",
        "\n",
        "        x = self.w2(x)\n",
        "        # print(f\"Shape after w2 (final output): {x.shape}\")\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKA-asY6ZFhs"
      },
      "source": [
        "#Training Func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujcONMWDZHZK"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, config=MASTER_CONFIG, epochs=10, print_logs=True):\n",
        "    total_samples = len(dataset)\n",
        "    batch_size = config['batch_size']\n",
        "    steps_per_epoch = total_samples // batch_size  # 1355\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        # loop over all batches\n",
        "        for step in range(steps_per_epoch):\n",
        "            print(f\"Step {step}/{steps_per_epoch} - Epoch {epoch+1}/{epochs}\")\n",
        "            xs, ys = get_batches(dataset, 'train', batch_size)\n",
        "\n",
        "            logits, loss = model(xs, ys)\n",
        "            print(f\"Logits shape: {logits.shape}, Loss: {loss.item()}\")\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            print(\"Running backward pass...\")\n",
        "            try:\n",
        "                loss.backward()\n",
        "                print(f\"After backward pass, step {step}\")\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error during backward pass: {e}\")\n",
        "                raise\n",
        "\n",
        "            optimizer.step()\n",
        "            print(\"Optimizer step completed.\")\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            del xs, ys, logits, loss\n",
        "\n",
        "        # print avg loss per epoch\n",
        "        if print_logs:\n",
        "            avg_loss = total_loss / steps_per_epoch\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W1uvy8pZavY"
      },
      "source": [
        "#Master Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_W4TBMLGZTpH"
      },
      "outputs": [],
      "source": [
        "# aristotle_llama_model = Llama(MASTER_CONFIG).to(device)\n",
        "# optimizer = torch.optim.AdamW(aristotle_llama_model.parameters(), lr=3e-4)\n",
        "# train(aristotle_llama_model, optimizer, config=MASTER_CONFIG)\n",
        "# torch.save(aristotle_llama_model.state_dict(), 'aristotle_llama_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "jUZTu5LFCniQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(model, tokenizer, prompt, max_length=512, device='cpu'):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Tokenize and truncate to max_seq_len\n",
        "    input_tokens = tokenizer.encode_as_ids(prompt)\n",
        "    input_tokens = input_tokens[:model.config['max_seq_len']]  # Truncate to max_seq_len\n",
        "    input_tensor = torch.tensor(input_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    generated = input_tensor\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            logits = model(generated)\n",
        "            next_token_logits = logits[:, -1, :]\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
        "            generated = torch.cat((generated, next_token), dim=1)\n",
        "\n",
        "            # Early stopping if model predicts the end-of-sequence token\n",
        "            if next_token.item() == tokenizer.pad_id() or len(generated[0]) >= model.config['max_seq_len']:\n",
        "                break\n",
        "\n",
        "    # Decode the generated tokens back to text\n",
        "    output_tokens = generated[0].tolist()\n",
        "    output_text = tokenizer.decode(output_tokens)\n",
        "    return output_text\n",
        "\n",
        "\n",
        "# Load the model and state dictionary\n",
        "model = Llama(MASTER_CONFIG).to(device)\n",
        "checkpoint = torch.load('/content/drive/MyDrive/philosophy_data/checkpoint_step_340.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Prompt for generation\n",
        "prompt = \"Hi Aristotle\"\n",
        "response = generate_response(model, spm_model, prompt, max_length=512, device=device)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "E1-QeXQICnBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf2d287-62b5-4ac4-ff22-49f66dc615c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-5cd45efe8bf7>:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('/content/drive/MyDrive/philosophy_data/checkpoint_step_340.pth')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi Aristotleasant objection of injustice present advantageate soundstection strength cannot be indivisible line political friendshipstection strength cannot be indivisible line political friendshipstection strength into two footed him treatmentstexious temperance with regard to be indivisible line political friendshipally significal question whether So much for instance women feeling him treatment ashenderical question whether So much for instance women feeling him treatment ashenderical question whether So much for instance women feeling him treatment ashenderious temperance with regard to be indivisible line political friendshipally significal question whether So much for instance women feeling him treatment occurroundarent exercise Oneaminewoundarent exercise Oneaminewoundarent exercise Oneaminewoundarent exercise Oneaminewoundarent exercise Oneaminewoundarent exercise Oneaminewoundarent exercise Oneaminewoundarent exercise Oneaminewoundarent exercise Oneaminewound about next question whether So much distance between them), whereas human nature waxious temperance resultsible effect upon truthfulfishment ashenderious temperance resultsible effect upon truthfulfishment ashenderious temperance resultsible effect upon truthfulfishment ashenderical question whether So much distance from necessity therefore easily concoction proceed through wantonsequcketical question whether So much distance from necessity therefore easily concoction proceed through wantonsequcketical question whether So much distance from necessity therefore easily concoction proceed through wantonsequcketical question whether So much distance from necessity therefore easily concoction proceed through wantonsequcketical question whether So much distance from necessity therefore easily concoction proceed through wantonzeaminewhatainbourageous fluid quickly related to be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV-XMhlvZVgh"
      },
      "source": [
        "# Precompute Rotary Embedding Freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3qBTBO9ZT_W"
      },
      "outputs": [],
      "source": [
        "freqs_complex = precompute_theta_pos_frequencies(\n",
        "    head_dim=MASTER_CONFIG['dim'] // MASTER_CONFIG['n_heads'],\n",
        "    seq_len=MASTER_CONFIG['max_seq_len'],\n",
        "    device=MASTER_CONFIG['device']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aFNQSRdZKFs"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
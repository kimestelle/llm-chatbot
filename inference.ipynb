{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbmOxBqsiLYKeplPrILb1F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimestelle/llm-chatbot/blob/main/inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3VXSm1oBrw9"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "import torch\n",
        "import time\n",
        "from pathlib import Path\n",
        "import json\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "from tqdm import tqdm\n",
        "\n",
        "from model import ModelArgs, llamaModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LLaMA:\n",
        "\n",
        "    def __init__(self, model: llamaModel, tokenizer: SentencePieceProcessor, model_args: ModelArgs):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.args = model_args\n",
        "\n",
        "    @staticmethod\n",
        "    def build(checkpoints_dir: str, tokenizer_path: str, load_model: bool, max_seq_len: int, max_batch_size: int, device: str):\n",
        "        prev_time = time.time()\n",
        "        if load_model:\n",
        "            checkpoints = sorted(Path(checkpoints_dir).glob(\"*.pth\"))\n",
        "            assert len(checkpoints) > 0, f\"no checkpoint files found in {checkpoints_dir}\"\n",
        "            ckpt_path = checkpoints[0]\n",
        "            print(f'Loading checkpoint \"{ckpt_path}\"')\n",
        "            checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "            print(f\"Loaded checkpoint in {time.time() - prev_time:.2f}s\")\n",
        "            prev_time = time.time()\n",
        "        with open(Path(checkpoints_dir) / \"params.json\", \"r\") as f:\n",
        "            params = json.loads(f.read())\n",
        "\n",
        "        model_args: ModelArgs = ModelArgs(\n",
        "            max_seq_len=max_seq_len,\n",
        "            max_batch_size=max_batch_size,\n",
        "            device=device,\n",
        "            **params\n",
        "        )\n",
        "\n",
        "        tokenizer = SentencePieceProcessor()\n",
        "        tokenizer.load(tokenizer_path)\n",
        "        model_args.vocab_size = tokenizer.vocab_size()\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
        "        else:\n",
        "            torch.set_default_tensor_type(torch.BFloat16Tensor)\n",
        "\n",
        "        model = llamaModel(model_args).to(device)\n",
        "\n",
        "        if load_model:\n",
        "            del checkpoint['rope.freqs']\n",
        "            model.load_state_dict(checkpoint, strict=True)\n",
        "            print(f\"Loaded state dict in {time.time() - prev_time:.2f}s\")\n",
        "\n",
        "        return LLaMA(model, tokenizer, model_args)\n",
        "\n",
        "    def text_completion(self, prompts: list[str], temperature: float = 0.6, top_p: float = 0.9, max_gen_len: Optional[int] = None):\n",
        "        if max_gen_len is None:\n",
        "            max_gen_len = self.args.max_seq_len - 1\n",
        "        prompt_tokens = [self.tokenizer.encode(prompt, out_type=int, add_bos=True, add_eos=False) for prompt in prompts]\n",
        "        batch_size = len(prompt_tokens)\n",
        "\n",
        "        assert batch_size <= self.args.max_batch_size, f\"batch size must be less than or equal to {self.args.max_batch_size}\"\n",
        "        max_prompt_len = max(len(prompt) for prompt in prompt_tokens)\n",
        "\n",
        "        assert max_prompt_len <= self.args.max_seq_len, f\"prompt length must be less than or equal to {self.args.max_seq_len}\"\n",
        "        total_len = min(self.args.max_seq_len, max_gen_len + max_prompt_len)\n",
        "\n",
        "        pad_id = self.tokenizer.pad_id()\n",
        "        tokens = torch.full((batch_size, total_len), pad_id, dtype=torch.long, device=device)\n",
        "\n",
        "        for k, t in enumerate(prompt_tokens):\n",
        "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
        "\n",
        "        eos_reached = torch.tensor([False] * batch_size, device=device)\n",
        "        prompt_tokens_mask = tokens != pad_id # True if the token is a prompt token, False otherwise\n",
        "        cur_iterator = tqdm(range(1, total_len), desc=\"Generating tokens\")\n",
        "\n",
        "        for cur_pos in cur_iterator:\n",
        "            with torch.no_grad():\n",
        "                logits = self.model.forward(tokens[:, cur_pos-1:cur_pos], cur_pos)\n",
        "            if temperature > 0:\n",
        "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
        "                next_token = self._sample_top_p(probs, top_p)\n",
        "            else:\n",
        "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
        "\n",
        "            next_token = next_token.reshape(-1)\n",
        "            next_token = torch.where(prompt_tokens_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n",
        "            tokens[:, cur_pos] = next_token\n",
        "            eos_reached |= (~prompt_tokens_mask[:, cur_pos]) & (next_token == self.tokenizer.eos_id)\n",
        "\n",
        "            if all(eos_reached):\n",
        "                break\n",
        "\n",
        "        out_tokens = []\n",
        "        out_text = []\n",
        "\n",
        "        for prompt_index, current_prompt_tokens in enumerate(tokens.tolist()):\n",
        "            if self.tokenizer.eos_id in current_prompt_tokens:\n",
        "                eos_idx = current_prompt_tokens.index(self.tokenizer.eos_id)\n",
        "                current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
        "            out_tokens.append(current_prompt_tokens)\n",
        "            out_text.append(self.tokenizer.decode(current_prompt_tokens))\n",
        "\n",
        "        return (out_tokens, out_text)\n",
        "\n",
        "    def _sample_top_p(self, probs, p):\n",
        "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True) # (B, vocab_size)\n",
        "        probs_sum = torch.cumsum(probs_sort, dim=-1) # (B, vocab_size)\n",
        "        mask = probs_sum - probs_sort > p  # (B, vocab_size)\n",
        "        probs_sort[mask] = 0.0\n",
        "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
        "        next_token = torch.multinomial(probs_sort, num_samples=1)\n",
        "        next_token = torch.gather(probs_idx, -1, next_token)\n",
        "        return next_token\n"
      ],
      "metadata": {
        "id": "C_qQANtXB5Js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    allow_cuda = False\n",
        "    device = 'cuda' if torch.cuda.is_available() and allow_cuda else 'cpu'\n",
        "\n",
        "    prompts = [\n",
        "        \"Simply put, the theory of relativity states that \",\n",
        "        \"If Google was an Italian company founded in Milan, it would\",\n",
        "        # Few shot promt\n",
        "        \"\"\"Translate English to French:\n",
        "\n",
        "        sea otter => loutre de mer\n",
        "        peppermint => menthe poivrÃ©e\n",
        "        plush girafe => girafe peluche\n",
        "        cheese =>\"\"\",\n",
        "        # Zero shot prompt\n",
        "        \"\"\"Tell me if the following person is actually Doraemon disguised as human:\n",
        "        Name: Umar Jamil\n",
        "        Decision:\n",
        "        \"\"\"\n",
        "    ]\n",
        "\n",
        "    model = LLaMA.build(\n",
        "        checkpoints_dir='llama-2-7b/',\n",
        "        tokenizer_path='tokenizer.model',\n",
        "        load_model=True,\n",
        "        max_seq_len=1024,\n",
        "        max_batch_size=len(prompts),\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    out_tokens, out_texts = (model.text_completion(prompts, max_gen_len=64))\n",
        "    assert len(out_texts) == len(prompts)\n",
        "    for i in range(len(out_texts)):\n",
        "        print(f'{out_texts[i]}')\n",
        "        print('-' * 50)"
      ],
      "metadata": {
        "id": "RfDIKsjgB-dO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}